---
layout: default
---
      <div class="contents">

         <h1>AISTATS*2010 Invited Speakers</h1>

         <img src="{{ site.baseurl }}/images/gill.jpg" alt="Richard Gill" align="left" width=100px>

<h2>Richard Gill</h2>
<h3>Mathematical Institute, Leiden University</h3>

<b>Forensic Statistics: Where Are We and Where are We Going?</b>

<p>I will discuss the present situation of Forensic statistics. Rapid
developments in forensic science are putting statistics and probability
more and more into the court-room lime-light, often with apalling results.
Why is this and where should we go? Standard Bayesian and standard
frequentist statistics are based on the wrong paradigms. Forensic
statisticians have to learn from the learning community. But in forensic
statistics, N=1. How can we learn?

<p><b>Bio:</b> Richard Gill has occupied the Chair of Mathematical
Statistics at the University of Leiden since 2006, having held other
posts in the Netherlands since 1974. His research interests are in
law, statistics in law, biostatistics, genetics, survival analysis,
semiparametric models, causality, machine learning, statistical image
analysis and quantum statistical information. He is fascinated by
foundational aspects of statistics, probability and quantum physics,
and by the societal role of science in general and of statistics in
particular.

<p>In his work on quantum information he applies ideas and methodology
from probability and statistics to experimental and technological
problems in quantum science. In the new biosciences he sees fantastic
challenges for probability and statistics, and he is especially
interested in statistical problems of forensic DNA profiling.

<p>He is an elected member of the Dutch Royal Academy of Sciences, he
is currently president of the Dutch Society for Statistics and
Operations Research, and he holds the Distinguished Lorentz Fellowship
for 2010-2011 for his contributions to forensic statistics. During
2007-2010 he was deeply involved in the movement to have the case of
Lucia de Berk reopened, fighting a miscarriage of justice that was
righted in April, 2010, as widely reported in the international press.


<img src="{{ site.baseurl }}/images/lafferty.jpg" alt="John Lafferty" align="left" width=100px>

<h2>John Lafferty</h2>
<h3> School of Computer Science,
                   Carnegie Mellon University</h3>
<b>Nonparametric Learning of Functions and Graphs in High Dimensions</b>


<p>We present recent work on several nonparametric learning problems in
the high dimensional setting.  In particular, we present theory and
methods for estimating sparse regression functions, additive models,
and graphical models.  For additive models, we present a functional
version of methods based on l1 regularization for linear models.  For
graphical models, we develop methods for estimating the underlying graph
based only on observations.  One approach is something we call "the
nonparanormal," which uses copula methods to transform the variables
by nonparametric functions, relaxing the strong distributional
assumptions made by the Gaussian graphical model.  Another approach is
to restrict the family of allowed graphs to spanning forests, enabling
the use of fully nonparametric density estimation.  All of the
approaches are easy to understand, simple to use, theoretically well
supported, and effective for modeling of high dimensional data.  Joint
work with Anupam Gupta, Han Liu, Larry Wasserman, and Min Xu.


<p><b>Bio:</b> John Lafferty is a professor in the Computer Science
Department and the Machine Learning Department within the School of
Computer Science at Carnegie Mellon University, where he also holds a
joint appointment in the Department of Statistics.  His Ph.D. is in
Mathematics from Princeton University, where he was part of the
Program in Applied and Computational Mathematics. Before joining CMU
he was a Research Staff Member at the IBM Watson Research Center in
Yorktown Heights, New York, where he first starting working at the
interface of AI and Statistics. His recent research interests are in
text analysis, machine learning, and statistical learning theory, with
a recent focus on nonparametric methods for high dimensional data.  He
has served as co-director of CMU's Ph.D. Program in Machine Learning,
and was recently paroled after serving a term as program co-chair of
the 2009 NIPS Conference.

<p><img src="{{ site.baseurl }}/images/tavare.jpg" alt="Simon Taver&eacute;" align="left" width=100px>


<h2>Simon Tavar&eacute;</h2>
<h3>Department of Applied Mathematics and Theoretical Physics, and Department of Oncology, University of Cambridge & Program in Molecular and Computational Biology, USC</h3>

<b>Approximate Bayesian Computation: What, Why and How?</b>

<p>Approximate Bayesian Computation (ABC) arose in response to the difficulty of simulating observations from posterior distributions determined by intractable likelihoods. The method exploits the fact that while likelihoods may be impossible to compute in complex probability models, it is often easy to simulate observations from them. ABC in its simplest form proceeds as follows:  (i) simulate a parameter from the prior; (ii) simulate observations from the model with this parameter; (iii) accept the parameter if the simulated observations are close enough to the observed data. The magic, and the source of potential disasters, is in step (iii). This talk will outline what we know (and don't!) about ABC and illustrate the methods with applications to the fossil record and stem cell biology.

 

 

 

 

<p><b>Bio: </b>Simon Tavar&eacute; has for many years worked on statistical problems arising in molecular biology, human genetics, population genetics, molecular evolution bioinformatics and computational biology.  Among his methodological interests is stochastic computation, including ABC, the topic of his lecture.

 

<p>He is a Professor in the Department of Applied Mathematics and Theoretical Physics and Professor of Cancer Research (Bioinformatics) in the Oncology Department at the University of Cambridge. He is also a Senior Group Leader in the new Cancer Research UK Cambridge Research Institute. His group there focuses mainly on cancer genomics and evolutionary approaches to cancer. In 2009 he was elected a Fellow of the Academy of Medical Sciences.

 

<p>Simon is also a Research Professor, and George and Louise Kawamoto Chair in Biological Sciences, at the University of Southern California. He is PI of the NIH Center of Excellence in Genomic Science at USC, which is developing computational and experimental approaches for understanding how genotype relates to phenotype.


 
<!--
<h2>William S. Cleveland</h2>
<h3>Department of Statistics, Purdue University</h3>
<b>Data Visualization: Retaining the Information in Data</b>
Data visualization is critical to data analysis. It retains the
information in the data.

If a statistical model fitted to a dataset is a poor approximation of
the patterns in the data, the ensuing inferences based on the model will
likely be incorrect.  Visualization is the critical methodology for model
checking and revision.  It is not sufficient to simply optimize a numeric
model selection criterion to find the best model in a class because it
is possible that no model in the class fits the data.

</p><p>If a machine learning method is to be used for a task such as
classification, an understanding of the patterns in the data through
visualization helps determine which classification methods are likely to
work best.  Using the best performer out of a number of machine learning
methods, without checking patterns, is not sufficient because it is
possible that all tested methods perform poorly compared with methods
not tried, including new methods developed on the spot to take advantage
of the patterns discovered by the data visualization.

</p><p>Visualization and numeric methods of statistics and machine learning
go hand in hand in the analysis of a set of data. The numeric methods
exploit the immense tactical power of the computer in analyzing the
data. Visualization exploits the immense strategic power of the human
in analyzing the data.  The combination provides the best chance to
preserve the information in the data. This combination was the basis of
the controversy that arose in Kasparov vs. the machine learning algorithm
Deep Blue.

</p><p>The merger of numeric methods and visualization is illustrated by ed,
a new method of nonparametric density estimation. There are two goals
in its design: estimation that adapts to a wide range of patterns in
data, and a mechanism that allows effective diagnostic checking with
visualization tools to see if ed did a good job.

</p><p>Very large datasets, ubiquitous today, challenge data visualization
as they do all areas involved in the analysis of data.  Comprehensive
visualization that preserves the information in the data requires a
visualization database (VDB): many displays, some with many pages and
with one or more panels per page. A single display typically results from
breaking the data into subsets, and then using the same graphical method
to plot each subset, one per panel.  A VDB is queried on an as needed
basis with a viewer. Some displays might be studied in their entirety;
for others, studying only a small fraction of the pages might suffice.
On-the-fly computation without storage does not generally succeed because
computation time is too large.

</p><p>The sizes and numbers of displays of VDBs require a rethinking all
areas involved in data visualization, including the following: Methods
of display design that enhance pattern perception to enable rapid page
scanning; Automation algorithms for basic display elements such as the
aspect ratio, scales across panels, line types and widths, and symbol
types and sizes; Methods for subset view selection; Viewers designed
for multi-panel, multi-page displays that work with different amounts
of physical screen space.

</p><p><b>Bio:</b>William S. Cleveland is the Shanti S. Gupta Distinguished Professor
of Statistics and Courtesy Professor of Computer Science at Purdue
University. His areas of methodological research are in statistics,
machine learning, and data visualization.

</p><p>Cleveland has analyzed data sets ranging from very small to very large in
his research in computer networking, homeland security, visual perception,
environmental science, healthcare engineering, and customer opinion
polling.  In the course of this work he has developed many new methods
that are widely used throughout engineering, science, medicine, and business.

</p><p>In 2002 he was selected as a Highly Cited Researcher by the American
Society for Information Science &amp; Technology in the newly formed
mathematics category. In 1996 he was chosen national Statistician of the
Year by the Chicago Chapter of the American Statistical Association.
He is a Fellow of the American Statistical Association, the Institute
of Mathematical Statistics, the American Association of the Advancement
of Science, and the International Statistical Institute.
<br>

<img src="speakers_files/carvalho.jpg" alt="Carlos Cleveland" align="left"> 
</p><h2>Carlos M. Carvalho</h2>
<h3>Booth School of Business, University of Chicago</h3>
<b>Handling Sparsity via the Horseshoe</b>
<p>In this talk, I will present a new approach to sparse-signal detection
called the horseshoe estimator.  The horseshoe is a close cousin of the
lasso in that it arises from the same class of multivariate scale
mixtures of normals, but that it is more robust alternative at handling
unknown sparsity patterns.  A theoretical framework is proposed for
understanding why the horseshoe is a better default sparsity estimator
than those that arise from powered-exponential priors.  Comprehensive
numerical evidence is presented to show that the difference in
performance can often be large.  Most importantly, I will show that the
horseshoe estimator corresponds quite closely to the answers one would
get if one pursued a full Bayesian model-averaging approach using a
point mass at zero for noise, and a continuous density for signals.
Surprisingly, this correspondence holds both for the estimator itself
and for the classification rule induced by a simple threshold applied to
the estimator.  For most of this talk I will study sparsity in the
simplified context of estimating a vector of normal means. It is here
that the lessons drawn from a comparison of different approaches for
modeling sparsity are most readily understood, but these lessons
generalize straightforwardly to more difficult problems--regression,
covariance regularization, function estimation--where many of the
challenges of modern statistics lie. This is join work with Nicholas
Polson and James Scott.
<br>
<br>
<b>Bio:</b> Dr. Carvalho is an assistant professor of econometrics and statistics at The
University of Chicago Booth School of Business. Before coming to
Chicago, he was part of the Department of Statistical Sciences at Duke
University, first as a Ph.D. student followed by a Post-doc under the
supervision of Professor Mike West. His research focuses on Bayesian
statistics in complex, high-dimensional problems with applications
ranging from finance to genetics. Some of his current interests include
work on large-scale factor models, graphical models, Bayesian model
selection, sequential Monte Carlo methods and stochastic volatility
models.
</p><hr>
<img src="speakers_files/fp1.jpg" alt="Mark Hansen" align="left"> 
<h2>Mark H. Hansen</h2>
<h3>Department of Statistics, UCLA</h3>
<b>Words to look at, words to listen to</b>
<p>I will report on some recent collaborative artworks that draw on
dynamic data sources. I will spend most of my time on Moveable Type,
a large installation for the lobby of the New York Times building in 
New York City (co-created with Ben Rubin, EAR Studio). In this case, the data 
sources include a feed of the Times' news stories, an hourly dump of their web access and 
search logs (a sample, suitably anonymized), and the complete archive back to 1851. 
I will also spend  time on a new work, Exits, which is part of the Terre Natale exhibition 
at the Cartier Foundation in Paris (co-created with Diller Scofidio + Renfro, Laura Kurgan
and Ben Rubin). The centerpiece of our installation is a large, circular
projection that tells the story of global human migration and its causes. 
<br><br>
</p><p><b>Bio:</b> Mark Hansen is an Associate Professor of Statistics at UCLA, where
he also holds joint appointments in the departments of Electrical Engineering
and Design|Media Art. He is currently serving as Co-PI at CENS, the Center
for Embedded Networked Sensing, an NSF STC devoted to research into
the design and deployment of sensor networks. 
<br>
-->
<!--<b>Bio:</b> -->
<!--
           <ul>
            <li><a href="http://www.cs.princeton.edu/~moses/">Moses Charikar</a>, Princeton University</li>
            <li><a href="http://stat.stanford.edu/~hastie/">Trevor J. Hastie</a>, Stanford University</li>
            <li><a href="http://www.stat.washington.edu/thompson/">Elizabeth Thompson</a>, University of Washington</li>                  
           </ul>
-->
      </p></div>

